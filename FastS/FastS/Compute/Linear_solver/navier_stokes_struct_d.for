C        Generated by TAPENADE     (INRIA, Ecuador team)
C  Tapenade 3.13 (r6666M) -  1 Mar 2018 15:30
C
c***********************************************************************
c     $Date: 2010-11-04 13:25:50 +0100 (Thu, 04 Nov 2010) $
c     $Revision: 64 $
c     $Author: IvanMary $
c***********************************************************************
      subroutine navier_stokes_struct_d(ndo, nidom, nbre_thread_actif, 
     &                   ithread, omp_mode, layer_mode, nbre_socket, 
     &                   socket , mx_synchro, lssiter_verif, nptpsi, 
     &                   nitcfg , nitrun    , first_it     , nb_pulse, 
     &                   flagcelln, param_int, param_real,
     &                   temps, tot, ijkv_sdm, 
     &                   ind_dm_zone, ind_dm_socket, ind_dm_omp,
     &                   socket_topology, lok, topo_omp, inddm_omp,
     &                   cfl, x, y, z, celln, rop_ssiter,
     &                   rop_ssiterd, krylov_in,
     &                   xmut, xmutd, 
     &                   ti, tj, tk,vol,ti_df,tj_df,tk_df,vol_df,
     &                   venti, ventj, ventk, wig, stat_wig, rot,
     &                   drodm, drodmd, coe, delta, ro_res)
c***********************************************************************
c_U   USER : GUEGAN   
c
C  Differentiation of navier_stokes_struct in forward (tangent) mode:
C   variations   of useful results: drodm
C   with respect to varying inputs: rop_ssiter
C   RW status of diff variables: drodm:out rop_ssiter:in
c***********************************************************************
      IMPLICIT NONE

#include "FastS/param_solver.h"

      INTEGER_E ndo, nidom, Nbre_thread_actif , mx_synchro, first_it,
     & ithread, ithread_io, Nbre_socket, socket, nitrun, nptpsi, nitcfg,
     & nb_pulse,
     & lssiter_verif,flagCellN,omp_mode,layer_mode
c
      INTEGER_E  ijkv_sdm(3),ind_dm_zone(6),
     & ind_dm_omp(6), ind_dm_socket(6), socket_topology(3),
     & param_int(0:*), topo_omp(3), inddm_omp(6), lok(*)

      REAL_E rop_ssiter(*), xmut(*), 
     & coe(*), ti(*), tj(*), tk(*), vol(*), x(*), y(*), 
     & z(*), venti(*), ventj(*), ventk(*), wig(*), stat_wig(*), 
     & rot(*), celln(*), ti_df(*), tj_df(*), tk_df(*), vol_df(*),
     & rop_ssiterd(*), krylov_in(*), drodm(*), drodmd(*),xmutd(*)

      REAL_E delta(*),ro_res(*)

      REAL_E psi(nptpsi)
      REAL_E temps, norm_kry, cfl(3), param_real(0:*)
      REAL_E drodmstk(20000,param_int(NEQ)), rostk(20000,param_int(NEQ))


C Var loc
#include "FastS/HPC_LAYER/LOC_VAR_DECLARATION.for"

      INTEGER_E tot(6,Nbre_thread_actif), totf(6), glob(4), 
     & ind_loop(6),neq_rot,depth,nb_bc


#include "FastS/formule_param.h"
#include "FastS/formule_mtr_param.h"
       
#include "FastS/HPC_LAYER/SIZE_MIN.for"
#include "FastS/HPC_LAYER/WORK_DISTRIBUTION_BEGIN.for"
#include "FastS/HPC_LAYER/LOOP_CACHE_BEGIN.for"
#include "FastS/HPC_LAYER/INDICE_RANGE.for"

         if(param_int(IFLOW).eq.3) then

           call vispalart_d(ndo, param_int, param_real, ind_grad,
     &                      xmut, xmutd, rop_ssiter, rop_ssiterd)
         endif

           !! a modifier pour generaliser
           call src_term_d(ndo, nitcfg, nb_pulse, param_int, param_real,
     &                     ind_sdm, ind_rhs, ind_ssa,
     &                     temps,
     &                     rop_ssiter, rop_ssiterd, xmut, drodm, drodmd,
     &                     coe, x,y,z,
     &                     ti,tj,tk,vol, delta)

#include "FastS/HPC_LAYER/SYNCHRO_WAIT.for"

          if(param_int(KFLUDOM).eq.5) then

            call fluroe_select_d(ndo, nitcfg, ithread, 
     &                        nptpsi, param_int, param_real,ind_dm_zone,
     &                        ind_sdm, ijkv_thread, ijkv_sdm, 
     &                        synchro_send_sock, synchro_send_th, 
     &                        synchro_receive_sock, synchro_receive_th, 
     &                        ibloc, jbloc, kbloc, 
     &                        icache, jcache, kcache, 
     &                        psi, wig, stat_wig, 
     &                        rop_ssiter, rop_ssiterd,
     &                        drodm, drodmd,
     &                        ti, ti_df, tj, tj_df, tk,tk_df,vol,vol_df,
     &                        venti, ventj, ventk, xmut, xmutd)

          elseif (ithread .eq. 1) then
              write(*, *) 'Unknown flux', param_int(KFLUDOM)
          endif

#include "FastS/HPC_LAYER/SYNCHRO_GO.for"

          !correction flux roe au CL si pas de mvt ALE,....
          nb_bc = param_int( param_int(PT_BC) )
          if(param_int(KFLUDOM).eq.5.and.nb_bc.ne.0) then

            call bfl3_d(ndo, ithread, param_int, 
     &                  param_real, ind_dm_zone, ind_sdm, 
     &                  psi, wig, stat_wig, rop_ssiter,
     &                  rop_ssiterd, drodm, drodmd,
     &                  ti, ti_df, tj, tj_df, tk, tk_df, 
     &                  vol, vol_df, venti, ventj, ventk , xmut, xmutd)

          endif
          if(param_int(ITYPCP).le.1.and.
     &        (param_int(IMPLICITSOLVER).eq.1.and.layer_mode.eq.1)) then
              !Assemble Residu Newton; 3q(n+1)-4Q(n)+q(n-1)) + dt (flu(i+1)-(flu(i)) =0
              if(flagCellN.eq.0) then

               call core3as2_kry_d(param_int, ind_mjr,drodm,drodmd, coe)
    
              else
                 if (ithread.eq.1)write(*, *) 
     &                'Unknown temporal scheme navier_d',
     &                param_int(IMPLICITSOLVER)
                 continue 
cc               call core3as2_chim_kry(ndo,nitcfg, first_it, 
cc     &                                param_int,param_real,
cc     &                                ind_mjr, cellN,
cc     &                                krylov, norm_kry,
cc     &                                rop_ssiter, rop, rop_m1,drodm,coe)
              endif
          endif



          !modifier plage d'indice comme pour mise a jour rk3 sur
          !vecteur unique: rop = rop + drodm et non rop_1 =rop +drodm
          !call id_vect(param_int,ind_mjr,drodmd, rop_ssiterd,krylov_in)

#include "FastS/HPC_LAYER/LOOP_CACHE_END.for"
#include "FastS/HPC_LAYER/WORK_DISTRIBUTION_END.for"

      END
